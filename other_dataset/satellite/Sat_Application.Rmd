---
title: "Satellite Data Application"
author: "Qi Wang"
date: "2023-05-11"
output: pdf_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(FRK)
library(spNNGP)
library(ggplot2)
library(maps)
library(MBA)
library(fields)
library(sp)
library(ncdf4)
library(reticulate)
use_condaenv("tf_gpu")
library(tensorflow)
library(keras)
library(geoR)

min_max_scale <- function(x)
{
  low <- range(x)[1]
  high <- range(x)[2]
  
  out <- (x - low)/(high - low)
  return(out)
  
}

load(here::here("other_dataset/satellite/AllSatelliteTemps.RData"))
sat_dat <- na.omit(all.sat.temps[,c(1,2,4)])
colnames(sat_dat) <- c("long","lat", "y")
long <- sat_dat$long
lat <- sat_dat$lat
y <- sat_dat$y

 #use_index <- which(lat <= 36.5 & lat >= 34 & long <= -92 & long >= -93 )
 # long <- long[use_index]
 # lat <- lat[use_index]
 # y <- y[use_index]
 # sat_dat <- sat_dat[use_index,]
 coordinates(sat_dat) <- ~ long + lat
 
 
num_fold <- 5
num_sample <- 100
pred_drop <- 0.1



set.seed(0)
train_index_all <- sample(1:num_fold, length(y), replace = T)


pred_dnn <- matrix(NA,nrow = length(y), ncol = num_sample)
pred_dk <- matrix(NA,nrow = length(y), ncol = num_sample)
pred_ck <- matrix(NA,nrow = length(y), ncol = num_sample)
pred_inla <- matrix(NA,nrow = length(y), ncol = num_sample)

```


```{r}
# Basis Function Generation


gridbasis1 <- auto_basis(mainfold = plane(), data = sat_dat, nres = 1, type = "Gaussian", regular = 1)
gridbasis2 <- auto_basis(mainfold = plane(), data = sat_dat, nres = 2, type = "Gaussian", regular = 1)
gridbasis3 <- auto_basis(mainfold = plane(), data = sat_dat, nres = 3, type = "Gaussian", regular = 1)

show_basis(gridbasis3) + 
  coord_fixed() +
  xlab("Longitude") +
  ylab("Latitude")

basis_1 <- matrix(NA, nrow = nrow(sat_dat), ncol = length(gridbasis1@fn))


for (i in 1:length(gridbasis1@fn)) {
  basis_1[,i] <- gridbasis1@fn[[i]](coordinates(sat_dat))
}

basis_2 <- matrix(NA, nrow = nrow(sat_dat), ncol = length(gridbasis2@fn))
for (i in 1:length(gridbasis2@fn)) {
  basis_2[,i] <- gridbasis2@fn[[i]](coordinates(sat_dat))
}

basis_3 <- matrix(NA, nrow = nrow(sat_dat), ncol = length(gridbasis3@fn))
for (i in 1:length(gridbasis3@fn)) {
   basis_3[,i] <- gridbasis3@fn[[i]](coordinates(sat_dat))
 }


# Redefine three layers of basis images
basis_use_1_2d <- basis_3[,1:ncol(basis_1)]

basis_use_2_2d <- basis_3[,(ncol(basis_1)+1):ncol(basis_2)]

basis_use_3_2d <- basis_3[,(ncol(basis_2)+1):ncol(basis_3)]

# First resolution
shape_row_1 <- length(table(gridbasis3@df[which(gridbasis3@df$res == 1) , 2 ]))
shape_col_1 <- length(table(gridbasis3@df[which(gridbasis3@df$res == 1) , 1 ]))
basis_arr_1 <-  array(NA, dim = c(nrow(sat_dat), shape_row_1, shape_col_1))

for (i in 1:nrow(sat_dat)) {
  basis_arr_1[i,,] <- matrix(basis_use_1_2d[i,], nrow = shape_row_1, ncol = shape_col_1, byrow = T)
}

# Second resolution
shape_row_2 <- length(table(gridbasis3@df[which(gridbasis3@df$res == 2) , 2 ]))
shape_col_2 <- length(table(gridbasis3@df[which(gridbasis3@df$res == 2) , 1 ]))
basis_arr_2 <- array(NA, dim = c(nrow(sat_dat), shape_row_2, shape_col_2))

for (i in 1:nrow(sat_dat)) {
  basis_arr_2[i,,] <- matrix(basis_use_2_2d[i,], nrow = shape_row_2, ncol = shape_col_2, byrow = T)
}


# Third resolution
shape_row_3 <- length(table(gridbasis3@df[which(gridbasis3@df$res == 3) , 2 ]))
shape_col_3 <- length(table(gridbasis3@df[which(gridbasis3@df$res == 3) , 1 ]))
basis_arr_3 <- array(NA, dim = c(nrow(sat_dat), shape_row_3, shape_col_3))

for (i in 1:nrow(sat_dat)) {
  basis_arr_3[i,,] <- matrix(basis_use_3_2d[i,], nrow = shape_row_3, ncol = shape_col_3, byrow = T)
}

pred_drop_layer <- layer_dropout(rate=pred_drop)

```
#DNN

```{r}

for (curr_index in 1:num_fold) {
  train_index <- which(train_index_all != curr_index)
  
  x_tr <- as.matrix( cbind(min_max_scale(long),min_max_scale(lat))  )[train_index,]
  x_te <- as.matrix( cbind(min_max_scale(long),min_max_scale(lat))  )[-train_index,]

  y_tr <- y[train_index]
  y_te <- y[-train_index]


  dnn_input_layer <- layer_input(shape = c(ncol(x_tr))) 

  dnn_output_layer <- dnn_input_layer %>% 
  layer_dense(units = 100, activation = 'relu',  kernel_initializer = "he_uniform") %>% 
  pred_drop_layer(training = T) %>%
  layer_batch_normalization()%>%
  layer_dense(units = 100, activation = 'relu') %>%
  pred_drop_layer(training = T) %>%
  layer_batch_normalization()%>%
  layer_dense(units = 100, activation = 'relu') %>%
  pred_drop_layer(training = T) %>%
  layer_batch_normalization()%>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'linear')

  
  model_dnn <- keras_model(dnn_input_layer, dnn_output_layer)
# Compile the model

model_dnn %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(),
  metrics = list("mse")
)
  model_checkpoint <- callback_model_checkpoint(
  filepath = "D:/77/research/temp/best_weights.h5",
  save_best_only = TRUE,
  monitor = "val_loss",
  mode = "min",
  verbose = 1
)
  
mod_train_dnn <- model_dnn %>%
  fit(x = x_tr, y = y_tr, epochs = 1000, batch_size = 1000, 
      validation_data = list(x_te, y_te) , callbacks = list(model_checkpoint))

model_dnn %>% load_model_weights_hdf5("D:/77/research/temp/best_weights.h5")


for (j in 1:num_sample) {
  print(j)
  pred_dnn[-train_index,j] <- predict(model_dnn, x_te)
}


}



```

# Deep Kriging

```{r}

for (curr_index in 1:num_fold) {
  
  train_index <- which(train_index_all != curr_index)
  basis_tr_1  <- basis_1[train_index,]
  basis_te_1  <- basis_1[-train_index,]
  
  basis_tr_2  <- basis_2[train_index,]
  basis_te_2  <- basis_2[-train_index,]
  
  basis_tr_3  <- basis_3[train_index,]
  basis_te_3  <- basis_3[-train_index,]
  x_tr <- cbind(as.matrix(cbind(min_max_scale(long),min_max_scale(lat)))[train_index,],basis_tr_1, basis_tr_2, basis_tr_3)
  x_te <- cbind(as.matrix(cbind(min_max_scale(long),min_max_scale(lat)))[-train_index,],basis_te_1, basis_te_2, basis_te_3)

  y_tr <- y[train_index]
  y_te <- y[-train_index]


  dk_input_layer <- layer_input(shape = c(ncol(x_tr))) 

  dk_output_layer <- dk_input_layer %>% 
  layer_dense(units = 100, activation = 'relu', kernel_initializer = "he_uniform") %>% 
  pred_drop_layer(training = T) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  pred_drop_layer(training = T) %>%
  layer_batch_normalization()%>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'linear')
  
  
  
model_dk <- keras_model(dk_input_layer, dk_output_layer)

# Compile the model

model_dk %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(),
  metrics = list("mse")
)

  model_checkpoint <- callback_model_checkpoint(
  filepath = "D:/77/research/temp/best_weights.h5",
  save_best_only = TRUE,
  monitor = "val_loss",
  mode = "min",
  verbose = 1
)
  
  
mod_train_dk <- model_dk %>%
  fit(x = x_tr, y = y_tr, epochs =500, batch_size = 1000, 
      validation_data = list(x_te, y_te) , callbacks = model_checkpoint)


model_dk %>% load_model_weights_hdf5("D:/77/research/temp/best_weights.h5")


for (j in 1:num_sample) {
  print(j)
  pred_dk[-train_index,j] <- predict(model_dk, x_te)
}

}

```

#Convolutional Kriging

```{r}

for (curr_index in 1:num_fold) {
  
train_index <- which(train_index_all != curr_index)
basis_tr_1 <- array_reshape(basis_arr_1[train_index,,], c(length(train_index), shape_row_1, shape_col_1, 1))
basis_tr_2 <- array_reshape(basis_arr_2[train_index,,], c(length(train_index), shape_row_2, shape_col_2, 1))
basis_tr_3 <- array_reshape(basis_arr_3[train_index,,], c(length(train_index), shape_row_3, shape_col_3, 1))

basis_te_1 <- array_reshape(basis_arr_1[-train_index,,], c(length(y) - length(train_index), shape_row_1, shape_col_1, 1))
basis_te_2 <- array_reshape(basis_arr_2[-train_index,,], c(length(y) - length(train_index), shape_row_2, shape_col_2, 1))
basis_te_3 <- array_reshape(basis_arr_3[-train_index,,], c(length(y) - length(train_index), shape_row_3, shape_col_3, 1))

cov_tr <- as.matrix( cbind(min_max_scale(long),min_max_scale(lat)) )[train_index,]
cov_te <- as.matrix( cbind(min_max_scale(long),min_max_scale(lat)) )[-train_index,]

# We need three convolutional input model and adding covariates.
input_basis_1 <- layer_input(shape = c(shape_row_1, shape_col_1, 1))
input_basis_2 <- layer_input(shape = c(shape_row_2, shape_col_2, 1))
input_basis_3 <- layer_input(shape = c(shape_row_3, shape_col_3, 1))
input_cov <- layer_input(shape = ncol(cov_tr))


resolution_1_conv <- input_basis_1 %>%
  layer_conv_2d(filters = 128, kernel_size = c(2,2), activation = 'relu') %>%
  layer_flatten() %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu')

  resolution_2_conv <- input_basis_2 %>%
  layer_conv_2d(filters = 128, kernel_size = c(2,2), activation = 'relu') %>%
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_flatten() %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu')
  
  resolution_3_conv <- input_basis_3 %>%
  layer_conv_2d(filters = 128, kernel_size = c(2,2), activation = 'relu') %>%
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_flatten() %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu')
  
  cov_model <- input_cov %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) %>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_batch_normalization() %>%
  pred_drop_layer(training = T) 


all_model <- layer_concatenate(list(resolution_1_conv, resolution_2_conv, resolution_3_conv, cov_model))

output_layer <- all_model %>%  layer_dense(units = 1, activation = 'linear')

model_ck <- keras_model(inputs = list(input_basis_1, input_basis_2, input_basis_3, input_cov), outputs = output_layer)

model_ck <- 
model_ck %>% compile(
  optimizer = "adam",
  loss = 'mse',
  metrics = c('mse')
)

 model_checkpoint <- callback_model_checkpoint(
  filepath = "D:/77/research/temp/best_weights.h5",
  save_best_only = TRUE,
  monitor = "val_loss",
  mode = "min",
  verbose = 1
)

mod_train_ck <- model_ck %>% fit(
  x = list(basis_tr_1, basis_tr_2, basis_tr_3, cov_tr),
  y = y[train_index],
  epochs=500,
  batch_size=2000,
  validation_data=list(list(basis_te_1,basis_te_2,basis_te_3,cov_te), y[-train_index]),
  callbacks = model_checkpoint, shuffle = TRUE
)

model_ck %>% load_model_weights_hdf5("D:/77/research/temp/best_weights.h5")

 for (j in 1:num_sample) {
  print(j)
  pred_ck[-train_index,j] <- predict(model_ck, list(basis_te_1,basis_te_2,basis_te_3,cov_te))
}
   
}




```


## INLA 
```{r}
library(INLA)
library(rSPDE)
library(gridExtra)
library(lattice)

# ml_idx <- sample(1:length(long), 2000)
# ml_res <- likfit(coords = cbind(long[ml_idx], lat[ml_idx]), data = y[ml_idx], ini.cov.pars = c(var(y)/2,sort(unique(diff(long)))[2]))
inla_range <- min( diff(range(long)), diff(range(lat)) ) / sqrt(length(long)) * 5

for (curr_index in 1:num_fold) {
  print(paste("Now doing index ", curr_index))
  train_index <- which(train_index_all != curr_index)
  long_tr <- long[train_index]
  lat_tr <- lat[train_index]
  y_tr <- y[train_index]
  coords <- cbind(long_tr, lat_tr)
  
  non_convex_bdry <- inla.nonconvex.hull(coords, -0.03, resolution = c(100, 100))
  mesh4 <- inla.mesh.2d(boundary = non_convex_bdry, max.edge=c(inla_range, inla_range*2))
  
  
  A<-inla.spde.make.A(mesh=mesh4,loc=as.matrix(coords))
  
  
  spde <- inla.spde2.matern(mesh4, alpha=0.5)
  iset <- inla.spde.make.index(name = "spatial.field", spde$n.spde)
  
  stk <- inla.stack(data=list(y=y_tr), #the response
                    
                    A=list(A,1),  #the A matrix; the 1 is included to make the list(covariates)
                    
                    effects=list(c(list(Intercept=1), #the Intercept
                                   iset),  #the spatial index
                                 #the covariates
                                 list(long = long_tr,
                                      lat = lat_tr)
                    ), 
                    
                    #this is a quick name so you can call upon easily
                    tag='dat')
  
  
  formula0<- y ~ -1 + long + lat + f(spatial.field, model=spde) 
  
  model0<-inla(formula0, #the formula
               data=inla.stack.data(stk,spde=spde),  #the data stack
               family= 'gaussian',   #which family the data comes from
               control.predictor=list(A=inla.stack.A(stk),compute=TRUE),  #compute gives you the marginals of the linear predictor
               control.compute = list(dic = TRUE, waic = TRUE, config = TRUE, return.marginals.predictor = TRUE), #model diagnostics and config = TRUE gives you the GMRF
                control.inla= list(strategy = 'simplified.laplace', huge = TRUE),
               verbose = FALSE) #can include verbose=TRUE to see the log of the model runs
  
  
  
  long_te <- long[-train_index]
  lat_te <- lat[-train_index]
  y_te <- y[-train_index]
  
  
  test_coords <- cbind(long_te, lat_te)
  
  
  # Prediction
  Aprediction <- inla.spde.make.A(mesh = mesh4, loc = test_coords);
  
  stk.pred <- inla.stack(data=list(y=NA), 
                         A=list(Aprediction,1), 
                         effects=list(c(list(Intercept=1)
                                        ,iset),
                                      list(
                                        long = long_te,
                                        lat = lat_te
                                      )
                         ), 
                         tag='pred')
  
  #join the prediction stack with the one for the full data
  stk.full <- inla.stack(stk, stk.pred)
  
  p.res.pred<-inla(formula0, data=inla.stack.data(stk.full,spde=spde),
                   family= 'gaussian', quantiles = NULL,
                   control.predictor=list(link = 1, A=inla.stack.A(stk.full),compute=TRUE),  #compute gives you the marginals of the linear predictor
                   control.compute = list(config = TRUE), #model diagnostics and config = TRUE gives you the GMRF
                   control.inla= list(strategy = 'simplified.laplace', huge = TRUE),  #this is to make it run faster
                   verbose = FALSE) 
  
  index.pred<-inla.stack.index(stk.full, "pred")$data
  
  # <- inla.posterior.sample(n = num_sample, p.res.pred)
  temp_pos_sample <- inla.posterior.sample(num_sample,p.res.pred)
  pred_inla[-train_index,] <- matrix(sapply(temp_pos_sample, function(lst) lst$latent[index.pred]), ncol = num_sample)
}



```



```{r}
write.csv(as.data.frame(pred_dnn),here::here("D:/77/Reasearch/temp/sat_pred/dnn_pred_sat.csv"),row.names = FALSE)
write.csv(as.data.frame(pred_dk),here::here("D:/77/Reasearch/temp/sat_pred/dk_pred_sat.csv"),row.names = FALSE)
write.csv(as.data.frame(pred_ck),here::here("D:/77/Reasearch/temp/sat_pred/ck_pred_sat.csv"),row.names = FALSE)
write.csv(as.data.frame(pred_inla),here::here("D:/77/Reasearch/temp/sat_pred/inla_pred_sat.csv"),row.names = FALSE)


```
# CRPS Calculation

```{r}
library(sp)
library(ggplot2)

load(here::here("other_dataset/satellite/AllSatelliteTemps.RData"))
sat_dat <- na.omit(all.sat.temps[,c(1,2,4)])
colnames(sat_dat) <- c("long","lat", "y")
long <- sat_dat$long
lat <- sat_dat$lat
y <- sat_dat$y

calculate_dist_between <- function(x){
  return( mean(spDists(matrix(x, ncol = 1)))  )
}

calculate_dist_true <- function(x, true_x){

  return( abs(x-true_x) )

}

crps <- function(prediction_matrix, true_value){
  print("Calculating between-sample distance . . .")
  between_sample <- apply(prediction_matrix, 1, calculate_dist_between)
  print("Calculating absolute error . . .")

  sample_residual <- rowMeans(apply(prediction_matrix, 2, calculate_dist_true, true_x = true_value))

  res <- 0.5 * between_sample - sample_residual
  return(res)
}


pred_dnn <- as.matrix(read.csv("D:/77/Reasearch/temp/sat_pred/dnn_pred_sat.csv"))
pred_dk <- as.matrix(read.csv("D:/77/Reasearch/temp/sat_pred/dk_pred_sat.csv"))
pred_ck <- as.matrix(read.csv("D:/77/Reasearch/temp/sat_pred/ck_pred_sat.csv"))
pred_inla <- as.matrix(read.csv("D:/77/Reasearch/temp/sat_pred/inla_pred_sat.csv"))

crps_dnn_all <- crps(pred_dnn,y)
crps_dk_all <- crps(pred_dk,y)
crps_ck_all <- crps(pred_ck,y)
crps_inla_all <- crps(pred_inla,y)



print(paste("CRPS of DNN is: ", mean(crps_dnn_all)))
print(paste("CRPS of DK is: ", mean(crps_dk_all)))
print(paste("CRPS of CK is: ", mean(crps_ck_all)))
print(paste("CRPS of INLA is: ", mean(crps_inla_all)))


write.csv(crps_dnn_all,"D:/77/Reasearch/temp/sat_pred/crps_dnn_sat.csv",row.names = FALSE)
write.csv(crps_dk_all,"D:/77/Reasearch/temp/sat_pred/crps_dk_sat.csv",row.names = FALSE)
write.csv(crps_ck_all,"D:/77/Reasearch/temp/sat_pred/crps_ck_sat.csv",row.names = FALSE)
write.csv(crps_inla_all,"D:/77/Reasearch/temp/sat_pred/crps_inla_sat.csv",row.names = FALSE)


```

# Interval Score Calculation

```{r}
int_score <- function(l,u,true_x,alpha = 0.05){
  out_1 <- u-l
  out_2 <- 2/alpha * (l-true_x) * ifelse(true_x < l, 1, 0)
  out_3 <- 2/alpha * (true_x-u) * ifelse(true_x > u, 1, 0)
  return(out_1 + out_2 + out_3)
}

all_interval_score <- function(prediction_sample, alpha = 0.05, true_x){
  num_all <- length(true_x)
 pb <- txtProgressBar(min = 0,      # Minimum value of the progress bar
                     max = num_all, # Maximum value of the progress bar
                     style = 3,    # Progress bar style (also available style = 1 and style = 2)
                     width = 50,   # Progress bar width. Defaults to getOption("width")
                     char = "=")   # Character used to create the bar
 
  
  all_score <- rep(NA, length(true_x))
 for (int_score_idx in 1:length(true_x)) {
   setTxtProgressBar(pb, int_score_idx)
    l <- quantile(prediction_sample[int_score_idx,], alpha/2)
    u <- quantile(prediction_sample[int_score_idx,], 1-alpha/2)
    all_score[int_score_idx] <- int_score(l = l, u = u, true_x = true_x[int_score_idx], alpha = alpha)
  }
 return(all_score)
}

int_score_dnn <- all_interval_score(prediction_sample = pred_dnn, true_x = y)
int_score_dk <- all_interval_score(prediction_sample = pred_dk, true_x = y)
int_score_ck <- all_interval_score(prediction_sample = pred_ck, true_x = y)
int_score_inla <- all_interval_score(prediction_sample = pred_inla, true_x = y)


print(paste("Interval Score of DNN is: ", mean(int_score_dnn)))
print(paste("Interval Score of DK is: ", mean(int_score_dk)))
print(paste("Interval Score of CK is: ", mean(int_score_ck )))
print(paste("Interval Score of INLA is: ", mean(int_score_inla )))



write.csv(int_score_dnn,"D:/77/Reasearch/temp/sat_pred/int_dnn_sat.csv",row.names = FALSE)
write.csv(int_score_dk,"D:/77/Reasearch/temp/sat_pred/int_dk_sat.csv",row.names = FALSE)
write.csv(int_score_ck,"D:/77/Reasearch/temp/sat_pred/int_ck_sat.csv",row.names = FALSE)
write.csv(int_score_inla,"D:/77/Reasearch/temp/sat_pred/int_inla_sat.csv",row.names = FALSE)

```










